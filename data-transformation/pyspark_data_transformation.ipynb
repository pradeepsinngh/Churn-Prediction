{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Data Transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using ML package to build models in PySpark. ML Package needs data be put in a (**label: Double, features: Vector**) DataFrame format with correspondingly named fields. We will be using:\n",
    "- *StringIndexer*, \n",
    "- *OneHotEncoderEstimator* and \n",
    "- *VectorAssembler* in PySpark ML Package to do so.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'pyspark_data_transformation.ipynb', 'data_preprocessing.ipynb', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"./\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teleco_data = spark.read.csv('../data/Telco-Customer-Churn.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check dataframe schema \n",
    "teleco_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace/ Drop Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replacing spaces with null values in total charges column\n",
    "dfWithEmptyReplaced = teleco_data.withColumn('TotalCharges', when(col('TotalCharges') == ' ', None).otherwise(col('TotalCharges')).cast(\"float\"))\n",
    "dfWithEmptyReplaced = dfWithEmptyReplaced.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replacing 'No internet service' to No for the following columns\n",
    "replace_cols = [ 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection','TechSupport','StreamingTV', 'StreamingMovies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace values\n",
    "for col_name in replace_cols:\n",
    "    dfwithNo = dfWithEmptyReplaced.withColumn(col_name, when(col(col_name)== \"No internet service\",\"No\").otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfwithNo.createOrReplaceTempView(\"datawrangling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Spark SQL to create categories \n",
    "df_wrangling = spark.sql(\"\"\"\n",
    "select distinct \n",
    "         customerID\n",
    "        ,gender\n",
    "        ,SeniorCitizen\n",
    "        ,Partner\n",
    "        ,Dependents\n",
    "        ,tenure\n",
    "        ,case when (tenure<=12) then \"Tenure_0-12\"\n",
    "              when (tenure>12 and tenure <=24) then \"Tenure_12-24\"\n",
    "              when (tenure>24 and tenure <=48) then \"Tenure_24-48\"\n",
    "              when (tenure>48 and tenure <=60) then \"Tenure_48-60\"\n",
    "              when (tenure>60) then \"Tenure_gt_60\"\n",
    "        end as tenure_group\n",
    "        ,PhoneService\n",
    "        ,MultipleLines\n",
    "        ,InternetService\n",
    "        ,OnlineSecurity\n",
    "        ,OnlineBackup\n",
    "        ,DeviceProtection\n",
    "        ,TechSupport\n",
    "        ,StreamingTV\n",
    "        ,StreamingMovies\n",
    "        ,Contract\n",
    "        ,PaperlessBilling\n",
    "        ,PaymentMethod\n",
    "        ,MonthlyCharges\n",
    "        ,TotalCharges\n",
    "        ,Churn\n",
    "    from datawrangling\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customerID: string, gender: string, SeniorCitizen: int, Partner: string, Dependents: string, tenure: int, tenure_group: string, PhoneService: string, MultipleLines: string, InternetService: string, OnlineSecurity: string, OnlineBackup: string, DeviceProtection: string, TechSupport: string, StreamingTV: string, StreamingMovies: string, Contract: string, PaperlessBilling: string, PaymentMethod: string, MonthlyCharges: double, TotalCharges: float, Churn: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to pandas dataframe to see correlations\n",
    "df_wrangling_pandas = df_wrangling.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>tenure</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015683</td>\n",
       "      <td>0.219874</td>\n",
       "      <td>0.102411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tenure</th>\n",
       "      <td>0.015683</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.246862</td>\n",
       "      <td>0.825880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <td>0.219874</td>\n",
       "      <td>0.246862</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.651065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalCharges</th>\n",
       "      <td>0.102411</td>\n",
       "      <td>0.825880</td>\n",
       "      <td>0.651065</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                SeniorCitizen    tenure  MonthlyCharges  TotalCharges\n",
       "SeniorCitizen        1.000000  0.015683        0.219874      0.102411\n",
       "tenure               0.015683  1.000000        0.246862      0.825880\n",
       "MonthlyCharges       0.219874  0.246862        1.000000      0.651065\n",
       "TotalCharges         0.102411  0.825880        0.651065      1.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation\n",
    "correlation = df_wrangling_pandas.corr()\n",
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation:\n",
    "[ML Package](https://spark.apache.org/docs/latest/ml-guide.html) in Spark wants data in this format: **label: Double, features: Vector**. We will use *StringIndexer*, *OneHotEncoderEstimator* and *VectorAssembler* in PySpark ML Package to do so.\n",
    "\n",
    "[Reference](https://spark.apache.org/docs/latest/ml-features.html):\n",
    "- *StringIndexer:* encodes a string column of labels to a column of label indices.\n",
    "- *OneHotEncoderEstimator:* maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values.\n",
    "- *VectorAssembler:* is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select on categorical Columns from dataset\n",
    "categoricalColumns = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract','PaperlessBilling','PaymentMethod']\n",
    "stages = [] # stages in our Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer and store it in 'label' column\n",
    "label_stringIdx = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transforming all features into a vector using VectorAssembler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = ['MonthlyCharges', 'TotalCharges']\n",
    "# add numerical + categorical columns into assembler inputs\n",
    "assemblerInputs = numericCols + [c + \"classVec\" for c in categoricalColumns]\n",
    "# assemble all features into a single vector and store it in 'feature' column\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "IDcols = ['customerID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a pipeline to transform dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Run the feature transformations.\n",
    "#  fit() computes feature statistics as needed.\n",
    "pipelineModel = pipeline.fit(df_wrangling)\n",
    "#  transform() actually transforms the features.\n",
    "dataset = pipelineModel.transform(df_wrangling)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols= [\"label\", \"features\"] + IDcols\n",
    "transformed_dataset = dataset.select(selectedcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|            features|customerID|\n",
      "+-----+--------------------+----------+\n",
      "|  0.0|(28,[0,1,3,6,7,10...|6497-TILVL|\n",
      "|  1.0|(28,[0,1,3,5,6,7,...|0691-JVSYA|\n",
      "|  0.0|(28,[0,1,3,4,5,6,...|8544-GOQSH|\n",
      "|  0.0|(28,[0,1,2,3,4,5,...|5172-MIGPM|\n",
      "|  0.0|(28,[0,1,2,3,5,6,...|4312-KFRXN|\n",
      "|  1.0|[69.9,69.90000152...|5875-YPQFJ|\n",
      "|  0.0|(28,[0,1,2,3,4,5,...|3916-NRPAP|\n",
      "|  0.0|(28,[0,1,3,4,5,10...|4598-ZADCK|\n",
      "|  0.0|(28,[0,1,2,3,4,5,...|4137-BTIKL|\n",
      "|  0.0|(28,[0,1,3,6,8,10...|1032-MAELW|\n",
      "|  0.0|(28,[0,1,2,3,4,5,...|5906-BFOZT|\n",
      "|  0.0|(28,[0,1,2,3,4,5,...|1335-NTIUC|\n",
      "|  0.0|(28,[0,1,3,4,5,6,...|8885-QSQBX|\n",
      "|  0.0|(28,[0,1,2,3,4,5,...|7504-UWHNB|\n",
      "|  1.0|(28,[0,1,2,3,5,6,...|8443-WVPSS|\n",
      "|  1.0|(28,[0,1,2,5,10,1...|2778-OCLGR|\n",
      "|  1.0|(28,[0,1,2,3,4,5,...|8627-EHGIP|\n",
      "|  0.0|(28,[0,1,3,6,7,21...|4182-BGSIQ|\n",
      "|  0.0|(28,[0,1,2,3,5,6,...|7929-SKFGK|\n",
      "|  0.0|(28,[0,1,2,3,4,5,...|5063-IUOKK|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# peek into transformed dataset\n",
    "transformed_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- customerID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema for transformed data\n",
    "transformed_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "dataset = transformed_dataset.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save transformed dataset\n",
    "dataset.to_csv('../data/transformed_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "name": "Churn_Model",
  "notebookId": 1478569293548518
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
